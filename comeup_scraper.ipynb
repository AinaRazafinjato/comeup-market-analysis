{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8b8132",
   "metadata": {},
   "source": [
    "# Comeup Services Scraper - Step by Step Analysis\n",
    "\n",
    "This notebook refactors the Comeup scraper into a progressive, functional approach for better understanding and debugging of the scraping process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd657a6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for web scraping, data manipulation, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad7036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f050062",
   "metadata": {},
   "source": [
    "## 2. Setup Session and Configuration\n",
    "\n",
    "Initialize the requests session, set headers, and define configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6453d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session configured successfully!\n",
      "Base URL: https://comeup.com\n",
      "User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://comeup.com\"\n",
    "\n",
    "# Create session with proper headers\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "})\n",
    "\n",
    "# Global variables for data collection\n",
    "scraped_services = []\n",
    "\n",
    "print(\"Session configured successfully!\")\n",
    "print(f\"Base URL: {BASE_URL}\")\n",
    "print(f\"User Agent: {session.headers['User-Agent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a0398",
   "metadata": {},
   "source": [
    "## 3. Define Data Extraction Functions\n",
    "\n",
    "Create individual functions for extracting different pieces of data from service pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf3cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def extract_seller_name(soup):\n",
    "    \"\"\"Extrait le nom du vendeur\"\"\"\n",
    "    selectors = [\n",
    "        '.seller-name',\n",
    "        '.username',\n",
    "        '[data-testid=\"seller-name\"]',\n",
    "        '.profile-username'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            return element.get_text(strip=True)\n",
    "    return \"N/A\"\n",
    "\n",
    "def extract_service_title(soup):\n",
    "    \"\"\"Extrait le titre du service\"\"\"\n",
    "    selectors = [\n",
    "        'h1',\n",
    "        '.service-title',\n",
    "        '[data-testid=\"service-title\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            return element.get_text(strip=True)\n",
    "    return \"N/A\"\n",
    "\n",
    "def categorize_by_keywords(text):\n",
    "    \"\"\"Catégorise un service basé sur des mots-clés\"\"\"\n",
    "    categories = {\n",
    "        'Marketing Digital': ['email', 'marketing', 'publicité', 'ads', 'seo', 'social media'],\n",
    "        'Rédaction': ['rédiger', 'écrire', 'contenu', 'article', 'blog'],\n",
    "        'Design': ['logo', 'design', 'graphique', 'bannière', 'visuel'],\n",
    "        'Développement': ['site web', 'application', 'développement', 'code'],\n",
    "        'Vidéo': ['montage', 'vidéo', 'animation', 'motion'],\n",
    "        'Audio': ['voix off', 'musique', 'audio', 'podcast'],\n",
    "        'Traduction': ['traduction', 'traduire', 'langue'],\n",
    "        'Business': ['business plan', 'stratégie', 'conseil']\n",
    "    }\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for category, keywords in categories.items():\n",
    "        if any(keyword in text_lower for keyword in keywords):\n",
    "            return category\n",
    "    \n",
    "    return 'Autre'\n",
    "\n",
    "def extract_category(soup):\n",
    "    \"\"\"Extrait la catégorie du service\"\"\"\n",
    "    selectors = [\n",
    "        '.breadcrumb li:last-child',\n",
    "        '.category-name',\n",
    "        '[data-testid=\"category\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            return element.get_text(strip=True)\n",
    "    \n",
    "    # Si pas trouvé, essayer de déduire de la description\n",
    "    title = extract_service_title(soup)\n",
    "    return categorize_by_keywords(title)\n",
    "\n",
    "print(\"Extraction functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbd5622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data extraction functions ready!\n"
     ]
    }
   ],
   "source": [
    "def extract_price(soup):\n",
    "    \"\"\"Extrait le prix du service\"\"\"\n",
    "    selectors = [\n",
    "        '.price',\n",
    "        '.service-price',\n",
    "        '[data-testid=\"price\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            price_text = element.get_text(strip=True)\n",
    "            price_match = re.search(r'(\\d+(?:[,\\.]\\d+)?)', price_text.replace('€', '').replace(',', '.'))\n",
    "            if price_match:\n",
    "                return float(price_match.group(1))\n",
    "    return 0.0\n",
    "\n",
    "def extract_rating(soup):\n",
    "    \"\"\"Extrait la note moyenne\"\"\"\n",
    "    selectors = [\n",
    "        '.rating',\n",
    "        '.stars',\n",
    "        '[data-testid=\"rating\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            rating_text = element.get_text(strip=True)\n",
    "            rating_match = re.search(r'(\\d+(?:[,\\.]\\d+)?)', rating_text)\n",
    "            if rating_match:\n",
    "                return float(rating_match.group(1).replace(',', '.'))\n",
    "    return 0.0\n",
    "\n",
    "def extract_sales_count(soup):\n",
    "    \"\"\"Extrait le nombre total de ventes\"\"\"\n",
    "    selectors = [\n",
    "        '.sales-count',\n",
    "        '.orders-completed',\n",
    "        '[data-testid=\"sales\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            sales_text = element.get_text(strip=True)\n",
    "            sales_match = re.search(r'(\\d+)', sales_text)\n",
    "            if sales_match:\n",
    "                return int(sales_match.group(1))\n",
    "    return 0\n",
    "\n",
    "def extract_reviews(soup):\n",
    "    \"\"\"Extrait les avis (positifs et négatifs)\"\"\"\n",
    "    reviews = {'positifs': 0, 'negatifs': 0}\n",
    "    \n",
    "    review_elements = soup.select('.review, .comment, .feedback')\n",
    "    \n",
    "    for review in review_elements:\n",
    "        review_text = review.get_text(strip=True).lower()\n",
    "        \n",
    "        positive_keywords = ['excellent', 'parfait', 'recommande', 'super', 'génial', 'satisfait']\n",
    "        negative_keywords = ['décevant', 'mauvais', 'problème', 'insatisfait', 'nul']\n",
    "        \n",
    "        if any(keyword in review_text for keyword in positive_keywords):\n",
    "            reviews['positifs'] += 1\n",
    "        elif any(keyword in review_text for keyword in negative_keywords):\n",
    "            reviews['negatifs'] += 1\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def extract_seller_since(soup):\n",
    "    \"\"\"Extrait depuis quand le vendeur est inscrit\"\"\"\n",
    "    selectors = [\n",
    "        '.member-since',\n",
    "        '.seller-since',\n",
    "        '[data-testid=\"member-since\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            return element.get_text(strip=True)\n",
    "    return \"N/A\"\n",
    "\n",
    "print(\"All data extraction functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0dbe0",
   "metadata": {},
   "source": [
    "## 4. Test Single Service Scraping\n",
    "\n",
    "Test the extraction functions on a single service URL to verify they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_service_data(service_url):\n",
    "    \"\"\"Extrait toutes les données d'un service spécifique\"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting data from: {service_url}\")\n",
    "        response = session.get(service_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract reviews data\n",
    "        reviews = extract_reviews(soup)\n",
    "        \n",
    "        # Compile all data\n",
    "        data = {\n",
    "            'url': service_url,\n",
    "            'nom_vendeur': extract_seller_name(soup),\n",
    "            'titre_service': extract_service_title(soup),\n",
    "            'categorie': extract_category(soup),\n",
    "            'prix': extract_price(soup),\n",
    "            'note': extract_rating(soup),\n",
    "            'nombre_vente_total': extract_sales_count(soup),\n",
    "            'avis_positifs': reviews['positifs'],\n",
    "            'avis_negatifs': reviews['negatifs'],\n",
    "            'vendeur_depuis': extract_seller_since(soup),\n",
    "            'date_scraping': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction de {service_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a sample URL (you'll need to replace with an actual Comeup service URL)\n",
    "test_url = \"https://comeup.com/fr/service/example\"  # Replace with actual URL\n",
    "print(\"Testing single service extraction...\")\n",
    "print(\"(Note: Replace test_url with an actual Comeup service URL to test)\")\n",
    "\n",
    "# Uncomment the lines below when you have a real URL to test\n",
    "# test_data = extract_service_data(test_url)\n",
    "# if test_data:\n",
    "#     for key, value in test_data.items():\n",
    "#         print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0c348",
   "metadata": {},
   "source": [
    "## 5. Create Category Scraping Function\n",
    "\n",
    "Build a function to scrape service URLs from category pages with pagination support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45830293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_services_list(category_url, max_pages=5):\n",
    "    \"\"\"Scrape une liste de services depuis une catégorie\"\"\"\n",
    "    services_urls = []\n",
    "    \n",
    "    print(f\"Scraping category: {category_url}\")\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        try:\n",
    "            url = f\"{category_url}?page={page}\"\n",
    "            print(f\"  Scraping page {page}...\")\n",
    "            \n",
    "            response = session.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Sélecteurs pour les liens de services\n",
    "            service_links = soup.select('a[href*=\"/service/\"]')\n",
    "            \n",
    "            page_services = 0\n",
    "            for link in service_links:\n",
    "                service_url = urljoin(BASE_URL, link['href'])\n",
    "                if service_url not in services_urls:\n",
    "                    services_urls.append(service_url)\n",
    "                    page_services += 1\n",
    "            \n",
    "            print(f\"    Found {page_services} new services on page {page}\")\n",
    "            \n",
    "            # If no services found, probably reached the end\n",
    "            if page_services == 0:\n",
    "                print(f\"    No new services found on page {page}, stopping pagination\")\n",
    "                break\n",
    "                \n",
    "            time.sleep(1)  # Pause entre les requêtes\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Erreur page {page}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"  Total services found: {len(services_urls)}\")\n",
    "    return services_urls\n",
    "\n",
    "# Test the function (replace with actual category URL)\n",
    "test_category = \"https://comeup.com/fr/best-services\"\n",
    "print(\"Testing category scraping...\")\n",
    "print(\"(Note: This will attempt to scrape actual URLs)\")\n",
    "\n",
    "# Uncomment to test with real URLs\n",
    "# test_services = scrape_services_list(test_category, max_pages=2)\n",
    "# print(f\"Found {len(test_services)} services\")\n",
    "# if test_services:\n",
    "#     print(\"First 5 URLs:\")\n",
    "#     for url in test_services[:5]:\n",
    "#         print(f\"  {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f27591",
   "metadata": {},
   "source": [
    "## 6. Build Complete Scraping Pipeline\n",
    "\n",
    "Combine all functions into a complete pipeline that scrapes multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ba366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_categories(category_urls, max_pages_per_category=3, max_services_per_category=10):\n",
    "    \"\"\"Pipeline complet de scraping pour plusieurs catégories\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    print(f\"Starting scraping pipeline for {len(category_urls)} categories\")\n",
    "    print(f\"Max pages per category: {max_pages_per_category}\")\n",
    "    print(f\"Max services per category: {max_services_per_category}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, category_url in enumerate(category_urls, 1):\n",
    "        print(f\"\\n[{i}/{len(category_urls)}] Processing category: {category_url}\")\n",
    "        \n",
    "        try:\n",
    "            # Get service URLs from this category\n",
    "            service_urls = scrape_services_list(category_url, max_pages_per_category)\n",
    "            \n",
    "            # Limit number of services to scrape per category\n",
    "            service_urls = service_urls[:max_services_per_category]\n",
    "            \n",
    "            # Extract data from each service\n",
    "            for j, service_url in enumerate(service_urls, 1):\n",
    "                print(f\"  [{j}/{len(service_urls)}] Extracting: {service_url}\")\n",
    "                \n",
    "                service_data = extract_service_data(service_url)\n",
    "                if service_data:\n",
    "                    service_data['source_category'] = category_url\n",
    "                    all_data.append(service_data)\n",
    "                    print(f\"    ✓ Success: {service_data['titre_service'][:50]}...\")\n",
    "                else:\n",
    "                    print(f\"    ✗ Failed to extract data\")\n",
    "                \n",
    "                time.sleep(2)  # Pause entre les requêtes\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing category {category_url}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"Scraping completed! Total services scraped: {len(all_data)}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Example usage (replace with actual URLs)\n",
    "categories_to_scrape = [\n",
    "    \"https://comeup.com/fr/best-services\",\n",
    "    \"https://comeup.com/fr/category/site-developpement\",\n",
    "]\n",
    "\n",
    "print(\"Scraping pipeline ready!\")\n",
    "print(\"Categories configured:\")\n",
    "for cat in categories_to_scrape:\n",
    "    print(f\"  - {cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e2711",
   "metadata": {},
   "source": [
    "## 7. Export Data to CSV\n",
    "\n",
    "Save the collected data to a CSV file and display summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_to_csv(data, filename=\"comeup_analysis.csv\"):\n",
    "    \"\"\"Exporte les données vers un fichier CSV\"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to export!\")\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    filepath = f\"c:\\\\Users\\\\ME-PC\\\\Desktop\\\\ERAYDIGITAL\\\\projects\\\\perso\\\\COMEUP\\\\{filename}\"\n",
    "    df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Data exported successfully to: {filepath}\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_summary_statistics(df):\n",
    "    \"\"\"Affiche des statistiques de résumé\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to analyze!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Total services scraped: {len(df)}\")\n",
    "    print(f\"Unique sellers: {df['nom_vendeur'].nunique()}\")\n",
    "    print(f\"Categories found: {df['categorie'].nunique()}\")\n",
    "    \n",
    "    print(f\"\\nPrice statistics:\")\n",
    "    print(f\"  Average price: €{df['prix'].mean():.2f}\")\n",
    "    print(f\"  Median price: €{df['prix'].median():.2f}\")\n",
    "    print(f\"  Min price: €{df['prix'].min():.2f}\")\n",
    "    print(f\"  Max price: €{df['prix'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\nRating statistics:\")\n",
    "    print(f\"  Average rating: {df['note'].mean():.2f}\")\n",
    "    print(f\"  Services with rating > 4: {len(df[df['note'] > 4])}\")\n",
    "    \n",
    "    print(f\"\\nTop 5 categories:\")\n",
    "    print(df['categorie'].value_counts().head())\n",
    "    \n",
    "    print(f\"\\nTop 5 sellers by number of services:\")\n",
    "    print(df['nom_vendeur'].value_counts().head())\n",
    "\n",
    "# Example of how to use these functions:\n",
    "print(\"Export and analysis functions ready!\")\n",
    "print(\"\\nTo use these functions:\")\n",
    "print(\"1. First run the scraping pipeline:\")\n",
    "print(\"   scraped_data = scrape_multiple_categories(categories_to_scrape)\")\n",
    "print(\"2. Then export the data:\")\n",
    "print(\"   df = export_data_to_csv(scraped_data)\")\n",
    "print(\"3. Finally display statistics:\")\n",
    "print(\"   display_summary_statistics(df)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805372fc",
   "metadata": {},
   "source": [
    "## 8. Data Analysis and Visualization\n",
    "\n",
    "Perform basic analysis on the scraped data with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675604d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(df):\n",
    "    \"\"\"Crée des visualisations des données scrapées\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to visualize!\")\n",
    "        return\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Comeup Services Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Price Distribution\n",
    "    axes[0, 0].hist(df['prix'], bins=20, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Distribution des Prix')\n",
    "    axes[0, 0].set_xlabel('Prix (€)')\n",
    "    axes[0, 0].set_ylabel('Nombre de services')\n",
    "    \n",
    "    # 2. Category Distribution\n",
    "    category_counts = df['categorie'].value_counts().head(10)\n",
    "    axes[0, 1].bar(range(len(category_counts)), category_counts.values, color='lightcoral')\n",
    "    axes[0, 1].set_title('Top 10 Catégories')\n",
    "    axes[0, 1].set_xlabel('Catégories')\n",
    "    axes[0, 1].set_ylabel('Nombre de services')\n",
    "    axes[0, 1].set_xticks(range(len(category_counts)))\n",
    "    axes[0, 1].set_xticklabels(category_counts.index, rotation=45, ha='right')\n",
    "    \n",
    "    # 3. Price vs Rating Scatter\n",
    "    axes[1, 0].scatter(df['prix'], df['note'], alpha=0.6, color='green')\n",
    "    axes[1, 0].set_title('Prix vs Note')\n",
    "    axes[1, 0].set_xlabel('Prix (€)')\n",
    "    axes[1, 0].set_ylabel('Note')\n",
    "    \n",
    "    # 4. Sales Distribution\n",
    "    axes[1, 1].hist(df['nombre_vente_total'], bins=15, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_title('Distribution des Ventes')\n",
    "    axes[1, 1].set_xlabel('Nombre de ventes')\n",
    "    axes[1, 1].set_ylabel('Nombre de services')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_top_performers(df):\n",
    "    \"\"\"Analyse les services les plus performants\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to analyze!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TOP PERFORMERS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Top services by sales\n",
    "    print(\"\\nTop 10 services by sales:\")\n",
    "    top_sales = df.nlargest(10, 'nombre_vente_total')[['titre_service', 'nom_vendeur', 'nombre_vente_total', 'prix', 'note']]\n",
    "    print(top_sales.to_string(index=False))\n",
    "    \n",
    "    # Top services by rating (with minimum sales)\n",
    "    high_sales = df[df['nombre_vente_total'] >= 5]  # At least 5 sales\n",
    "    if not high_sales.empty:\n",
    "        print(f\"\\nTop 10 services by rating (min 5 sales):\")\n",
    "        top_rated = high_sales.nlargest(10, 'note')[['titre_service', 'nom_vendeur', 'note', 'nombre_vente_total', 'prix']]\n",
    "        print(top_rated.to_string(index=False))\n",
    "    \n",
    "    # Price analysis by category\n",
    "    print(f\"\\nAverage price by category:\")\n",
    "    price_by_category = df.groupby('categorie')['prix'].agg(['mean', 'count']).round(2)\n",
    "    price_by_category = price_by_category.sort_values('mean', ascending=False)\n",
    "    print(price_by_category.to_string())\n",
    "\n",
    "def generate_insights(df):\n",
    "    \"\"\"Génère des insights sur les données\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to analyze!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BUSINESS INSIGHTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Price insights\n",
    "    low_price = df['prix'].quantile(0.25)\n",
    "    high_price = df['prix'].quantile(0.75)\n",
    "    print(f\"Price segments:\")\n",
    "    print(f\"  Low price (bottom 25%): ≤ €{low_price:.2f}\")\n",
    "    print(f\"  High price (top 25%): ≥ €{high_price:.2f}\")\n",
    "    \n",
    "    # Success factors\n",
    "    successful_services = df[df['nombre_vente_total'] >= df['nombre_vente_total'].quantile(0.8)]\n",
    "    if not successful_services.empty:\n",
    "        print(f\"\\nCharacteristics of top 20% sellers:\")\n",
    "        print(f\"  Average price: €{successful_services['prix'].mean():.2f}\")\n",
    "        print(f\"  Average rating: {successful_services['note'].mean():.2f}\")\n",
    "        print(f\"  Most common categories:\")\n",
    "        print(successful_services['categorie'].value_counts().head(3).to_string())\n",
    "    \n",
    "    # Category opportunities\n",
    "    print(f\"\\nCategory analysis:\")\n",
    "    category_stats = df.groupby('categorie').agg({\n",
    "        'prix': 'mean',\n",
    "        'note': 'mean',\n",
    "        'nombre_vente_total': 'mean'\n",
    "    }).round(2)\n",
    "    category_stats['service_count'] = df['categorie'].value_counts()\n",
    "    category_stats = category_stats.sort_values('prix', ascending=False)\n",
    "    print(category_stats.head(10).to_string())\n",
    "\n",
    "print(\"Visualization and analysis functions ready!\")\n",
    "print(\"\\nTo create visualizations after scraping:\")\n",
    "print(\"  create_visualizations(df)\")\n",
    "print(\"  analyze_top_performers(df)\")\n",
    "print(\"  generate_insights(df)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814db8b",
   "metadata": {},
   "source": [
    "## Complete Execution Example\n",
    "\n",
    "Here's how to run the complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac3889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete execution example (uncomment to run with real data)\n",
    "\n",
    "def run_complete_analysis():\n",
    "    \"\"\"Exécute l'analyse complète\"\"\"\n",
    "    print(\"Starting complete Comeup analysis...\")\n",
    "    \n",
    "    # 1. Define categories to scrape\n",
    "    categories = [\n",
    "        \"https://comeup.com/fr/best-services\",\n",
    "        \"https://comeup.com/fr/category/site-developpement\",\n",
    "    ]\n",
    "    \n",
    "    # 2. Scrape data (limit for testing)\n",
    "    print(\"Step 1: Scraping data...\")\n",
    "    scraped_data = scrape_multiple_categories(\n",
    "        categories, \n",
    "        max_pages_per_category=2, \n",
    "        max_services_per_category=5\n",
    "    )\n",
    "    \n",
    "    if not scraped_data:\n",
    "        print(\"No data scraped! Check your URLs and internet connection.\")\n",
    "        return\n",
    "    \n",
    "    # 3. Export to CSV\n",
    "    print(\"\\nStep 2: Exporting data...\")\n",
    "    df = export_data_to_csv(scraped_data, \"comeup_analysis_complete.csv\")\n",
    "    \n",
    "    # 4. Display statistics\n",
    "    print(\"\\nStep 3: Analyzing data...\")\n",
    "    display_summary_statistics(df)\n",
    "    \n",
    "    # 5. Create visualizations\n",
    "    print(\"\\nStep 4: Creating visualizations...\")\n",
    "    create_visualizations(df)\n",
    "    \n",
    "    # 6. Analyze top performers\n",
    "    analyze_top_performers(df)\n",
    "    \n",
    "    # 7. Generate insights\n",
    "    generate_insights(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Instructions for execution\n",
    "print(\"EXECUTION INSTRUCTIONS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Ensure you have valid Comeup URLs in the categories list\")\n",
    "print(\"2. Check your internet connection\")\n",
    "print(\"3. Be mindful of the website's robots.txt and terms of service\")\n",
    "print(\"4. Start with small limits (few pages, few services) for testing\")\n",
    "print(\"5. Uncomment and run: df = run_complete_analysis()\")\n",
    "print(\"\\nIMPORTANT: Replace example URLs with actual Comeup service URLs!\")\n",
    "\n",
    "# Uncomment the line below to run the complete analysis\n",
    "# df = run_complete_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29054d04",
   "metadata": {},
   "source": [
    "## Notes and Best Practices\n",
    "\n",
    "- Always respect the website's robots.txt and terms of service\n",
    "- Use appropriate delays between requests to avoid overloading the server\n",
    "- Test with small datasets first before running large scraping operations\n",
    "- Handle errors gracefully and implement retry logic for production use\n",
    "- Consider using proxy rotation for large-scale scraping\n",
    "- Monitor your scraping performance and adjust parameters accordingly\n",
    "\n",
    "The notebook is now ready for progressive execution. Each cell can be run independently to test specific functionality before running the complete pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (first_kernel)",
   "language": "python",
   "name": "first_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
